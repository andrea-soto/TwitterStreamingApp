{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts for Twitter Streaming Application\n",
    "\n",
    "Description: This notebook was used to create or modified the scripts for this application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Storm Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storm Application Files\n",
    "|File Name|Location|Description|\n",
    "|:--|:-|:--|\n",
    "|tweets.py|./EX2Tweetwordcount/src/spouts/|Spout to collect tweets|\n",
    "|parse.py|./EX2Tweetwordcount/src/bolts/|Bolt to parse tweet and clean words|\n",
    "|wordcount.py|./EX2Tweetwordcount/src/bolts/|Bolt to count words and update Postgres|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweets.py (spout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting EX2Tweetwordcount/src/spouts/tweets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile EX2Tweetwordcount/src/spouts/tweets.py\n",
    "from __future__ import absolute_import, print_function, unicode_literals\n",
    "#TEST THIS\n",
    "import itertools, time\n",
    "import tweepy, copy \n",
    "import Queue, threading\n",
    "\n",
    "from streamparse.spout import Spout\n",
    "\n",
    "################################################################################\n",
    "# Twitter credentials\n",
    "################################################################################\n",
    "twitter_credentials = {\n",
    "}\n",
    "\n",
    "def auth_get(auth_key):\n",
    "    if auth_key in twitter_credentials:\n",
    "        return twitter_credentials[auth_key]\n",
    "    return None\n",
    "\n",
    "################################################################################\n",
    "# Class to listen and act on the incoming tweets\n",
    "################################################################################\n",
    "class TweetStreamListener(tweepy.StreamListener):\n",
    "\n",
    "    def __init__(self, listener):\n",
    "        self.listener = listener\n",
    "        super(self.__class__, self).__init__(listener.tweepy_api())\n",
    "\n",
    "    def on_status(self, status):\n",
    "        self.listener.queue().put(status.text, timeout = 0.01)\n",
    "        return True\n",
    "  \n",
    "    def on_error(self, status_code):\n",
    "        return True # keep stream alive\n",
    "  \n",
    "    def on_limit(self, track):\n",
    "        return True # keep stream alive\n",
    "\n",
    "class Tweets(Spout):\n",
    "\n",
    "    def initialize(self, stormconf, context):\n",
    "        self._queue = Queue.Queue(maxsize = 100)\n",
    "\n",
    "        consumer_key = auth_get(\"consumer_key\") \n",
    "        consumer_secret = auth_get(\"consumer_secret\") \n",
    "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "        if auth_get(\"access_token\") and auth_get(\"access_token_secret\"):\n",
    "            access_token = auth_get(\"access_token\")\n",
    "            access_token_secret = auth_get(\"access_token_secret\")\n",
    "            auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "        self._tweepy_api = tweepy.API(auth)\n",
    "\n",
    "        # Create the listener for twitter stream\n",
    "        listener = TweetStreamListener(self)\n",
    "\n",
    "        # Create the stream and listen for english tweets\n",
    "        stream = tweepy.Stream(auth, listener, timeout=None)\n",
    "        stream.filter(languages=[\"en\"], track=[\"a\", \"the\", \"i\", \"you\", \"u\"], async=True)\n",
    "\n",
    "    def queue(self):\n",
    "        return self._queue\n",
    "\n",
    "    def tweepy_api(self):\n",
    "        return self._tweepy_api\n",
    "\n",
    "    def next_tuple(self):\n",
    "        try:\n",
    "            tweet = self.queue().get(timeout = 0.1) \n",
    "            if tweet:\n",
    "                self.queue().task_done()\n",
    "                self.emit([tweet])\n",
    " \n",
    "        except Queue.Empty:\n",
    "            self.log(\"Empty queue exception\")\n",
    "            time.sleep(0.1) \n",
    "\n",
    "    def ack(self, tup_id):\n",
    "        pass  # if a tuple is processed properly, do nothing\n",
    "\n",
    "    def fail(self, tup_id):\n",
    "        pass  # if a tuple fails to process, do nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse.py (bolt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/bolts/parse.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile EX2Tweetwordcount/src/bolts/parse.py \n",
    "from __future__ import absolute_import, print_function, unicode_literals\n",
    "\n",
    "import re\n",
    "from streamparse.bolt import Bolt\n",
    "\n",
    "################################################################################\n",
    "# Function to check if the string contains only ascii chars\n",
    "################################################################################\n",
    "def ascii_string(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "class ParseTweet(Bolt):\n",
    "\n",
    "    def process(self, tup):\n",
    "        tweet = tup.values[0]  # extract the tweet\n",
    "\n",
    "        # Split the tweet into words\n",
    "        words = tweet.split()\n",
    "\n",
    "        # Filter out the hash tags, RT, @ and urls\n",
    "        valid_words = []\n",
    "        for word in words:\n",
    "\n",
    "            # Filter the hash tags\n",
    "            if word.startswith(\"#\"): continue\n",
    "\n",
    "            # Filter the user mentions\n",
    "            if word.startswith(\"@\"): continue\n",
    "\n",
    "            # Filter out retweet tags\n",
    "            if word.startswith(\"RT\"): continue\n",
    "\n",
    "            # Filter out the urls\n",
    "            if word.startswith(\"http\"): continue\n",
    "\n",
    "            # Strip leading and lagging punctuations\n",
    "            aword = word.strip(\"\\\"?><,'.:;)\")\n",
    "            # Clean other charactes from string \n",
    "            aword = aword.lower()\n",
    "            # Basic word cleaning\n",
    "            aword = re.sub(\"'\",\"\",aword)\n",
    "            aword = re.sub(\"/\",\"\", aword)\n",
    "            aword = re.sub(\"\\)\",\"\", aword)\n",
    "            aword = re.sub(\"\\(\",\"\", aword)\n",
    "            aword = re.sub(\"[0-9!@#$%^&*-_+=~{}|:;<>?,.]\",\"\", aword)\n",
    "            aword = aword.replace(\"\\\\\",\"\")\n",
    "\n",
    "            # now check if the word contains only ascii\n",
    "            if len(aword) > 0 and ascii_string(word):\n",
    "                valid_words.append([aword])\n",
    "\n",
    "        if not valid_words: return\n",
    "\n",
    "        # Emit all the words\n",
    "        self.emit_many(valid_words)\n",
    "\n",
    "        # tuple acknowledgement is handled automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wordcount.py (bolt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting EX2Tweetwordcount/src/bolts/wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile EX2Tweetwordcount/src/bolts/wordcount.py\n",
    "from __future__ import absolute_import, print_function, unicode_literals\n",
    "\n",
    "from collections import Counter\n",
    "from redis import StrictRedis\n",
    "from streamparse.bolt import Bolt\n",
    "\n",
    "import psycopg2\n",
    "import time\n",
    "\n",
    "class WordCounter(Bolt):\n",
    "\n",
    "    def initialize(self, conf, ctx):\n",
    "        self.counts = Counter()\n",
    "        self.redis = StrictRedis()\n",
    "\n",
    "    def process(self, tup):\n",
    "        word = tup.values[0]\n",
    "\n",
    "        # Write codes to increment the word count in Postgres\n",
    "        # Use psycopg to interact with Postgres\n",
    "        # Database name: Tcount \n",
    "        # Table name: Tweetwordcount \n",
    "        \n",
    "        # Connect to database\n",
    "        conn = psycopg2.connect(database=\"tcount\", user=\"postgres\")\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # Increment the local count\n",
    "        self.counts[word] += 1\n",
    "        \n",
    "        if self.counts[word] == 1:\n",
    "            # New word > INSERT INTO\n",
    "            sql = \"INSERT INTO Tweetwordcount (word,count) VALUES ('%s', %d);\" %(unicode(word), self.counts[word])\n",
    "        else:\n",
    "            # Update word count > UPDATE\n",
    "            sql = \"UPDATE Tweetwordcount SET count=%d WHERE word='%s';\" %(self.counts[word], unicode(word))\n",
    "\n",
    "        cur.execute(sql)\n",
    "        conn.commit()\n",
    "        #self.emit([word, self.counts[word]])\n",
    "\n",
    "        # Log the count - just to see the topology running\n",
    "        #self.log('%s: %d' % (word, self.counts[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Serving Scripts\n",
    "\n",
    "### Servicing Files (Query Postgres Database)\n",
    "|File Name|Location|Description|\n",
    "|:--|:--|:--|\n",
    "|finalresults.py|./analysis/|Return the count of a word. If a word is not provided, it returns all word counts|\n",
    "|histogram.py|./analysis/|Return all words with count between a given interval|\n",
    "|top20.py|./analysis/|Return the top-20 words by count and create a bar-chart saved as 'plot.png'|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finalresults.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting analysis/finalresults.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile analysis/finalresults.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import psycopg2\n",
    "import time\n",
    "import sys\n",
    "   \n",
    "def get_wordCount(word, show=25):\n",
    "    # Get count for the input word or all word counts. \n",
    "    # The output is restricted to the 'show' value.\n",
    "    \n",
    "    # Connect to database\n",
    "    conn = psycopg2.connect(database=\"tcount\", user=\"postgres\")\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Get total words counted in tweets\n",
    "    sql = \"SELECT count(*) FROM Tweetwordcount ;\" \n",
    "    cur.execute(sql)\n",
    "    total_words = cur.fetchall()\n",
    "    conn.commit()\n",
    "    \n",
    "    # Get count for word or all words (depending on input)\n",
    "    if word == None:\n",
    "        sql = \"SELECT * FROM Tweetwordcount ORDER BY word ASC LIMIT %d;\"%(min(show,total_words[0][0]))        \n",
    "    else:\n",
    "        sql = \"SELECT * FROM Tweetwordcount WHERE word='%s';\" %(word)\n",
    "    \n",
    "    cur.execute(sql)\n",
    "    result = cur.fetchall()\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    # Print results\n",
    "    if word == None:\n",
    "        print 'First %d word-counts (out of %d words):\\n'%(min(show,total_words[0][0]),total_words[0][0])\n",
    "        print \"%12s  %s\"%('Word','Count')\n",
    "        for w,c in result:\n",
    "            print \"%12s  %d\"%(w,c)\n",
    "    else:\n",
    "        print \"Number of occurences of '%s':  %d\"%(result[0][0], result[0][1]), \"\\t@\", time.ctime(time.time())\n",
    "\n",
    "# ===================================================================================\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    To get the number of occurances of single word:\n",
    "       python finalresults.py hello\n",
    "    \n",
    "    Get all the word counts, sorted alphabetically, one per line:\n",
    "       python finalresults.py\n",
    "    \n",
    "    The default is to show only the first 25 words. \n",
    "    To increase the number of words shown, send any number as an input:\n",
    "       python finalresults.py 2000\n",
    "    '''    \n",
    "    \n",
    "    numToShow = 25\n",
    "    word = None\n",
    "    # Get input word if any\n",
    "    if len(sys.argv) > 1:\n",
    "        # Get target word\n",
    "        if sys.argv[1].isdigit():\n",
    "            numToShow = int(sys.argv[1])\n",
    "        else:\n",
    "            word = sys.argv[1]\n",
    "    \n",
    "    get_wordCount(word, numToShow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### histogram.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting analysis/histogram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile analysis/histogram.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import psycopg2\n",
    "import time\n",
    "import sys\n",
    "   \n",
    "def histogram(min_frq, max_frq, show):\n",
    "        \n",
    "    # Connect to database\n",
    "    conn = psycopg2.connect(database=\"tcount\", user=\"postgres\")\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Get total words with count within interval\n",
    "    sql = \"SELECT count(*) FROM Tweetwordcount WHERE count >= %d and count <= %d;\"%(min_frq, max_frq)\n",
    "    cur.execute(sql)\n",
    "    total_words = cur.fetchall()\n",
    "    conn.commit()\n",
    "    \n",
    "    # Get words with count within interval\n",
    "    sql = \"SELECT * FROM Tweetwordcount WHERE \"\n",
    "    sql += \"count >= %d and count <= %d ORDER BY count ASC LIMIT %d;\"%(min_frq, max_frq, show)\n",
    "    \n",
    "    cur.execute(sql)\n",
    "    result = cur.fetchall()\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    # Print results\n",
    "    print 'Reporting %d words (out of %d words):\\n'%(min(show,total_words[0][0]),total_words[0][0])\n",
    "    print \"%12s  %s\"%('Word','Count')\n",
    "    for w,c in result:\n",
    "        print \"%12s  %d\"%(w,c)\n",
    "\n",
    "# ===================================================================================\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    Get all the words with count between MIN and MAX value provided:\n",
    "       python histogram.py 100 400\n",
    "    \n",
    "    The default is to show only the first 25 words. \n",
    "    To increase the number of words shown, send the number to show as a 3rd parameter:\n",
    "       python histogram.py 100 400 50\n",
    "    '''    \n",
    "    \n",
    "    numToShow = 25\n",
    "    if len(sys.argv) < 3:\n",
    "        print \"Few inputs - MIN and MAX values for count interval are requiered\"\n",
    "    else:\n",
    "        minVal = int(sys.argv[1])\n",
    "        maxVal = int(sys.argv[2])\n",
    "        if len(sys.argv) == 4:\n",
    "            numToShow = int(sys.argv[3])\n",
    "        if minVal > maxVal:\n",
    "            print \"min value provided is greater than max value. Values will be switched.\"\n",
    "    \n",
    "    histogram(minVal, maxVal, numToShow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top20.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting analysis/top20.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile analysis/top20.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "def bar_plot(names, values):    \n",
    "    \n",
    "    n = np.arange(len(values))\n",
    "    plt.figure(figsize=(14,7))\n",
    "    plt.title('Top-20 Words by Count\\n', size=18)\n",
    "    plt.ylabel(\"Occurrences (count)\", size=14)\n",
    "    plt.bar(n, values, color='#89C6DA')\n",
    "    plt.xticks(n+0.5, names, rotation=0, size=12)\n",
    "    \n",
    "    plt.savefig('analysis/plot.png', format='png')\n",
    "    \n",
    "def top20():\n",
    "        \n",
    "    # Connect to database\n",
    "    conn = psycopg2.connect(database=\"tcount\", user=\"postgres\")\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Get total word count\n",
    "    sql = \"SELECT count(count) FROM Tweetwordcount;\"\n",
    "    cur.execute(sql)\n",
    "    total_count = cur.fetchall()\n",
    "    conn.commit()\n",
    "    \n",
    "    # Get top-20\n",
    "    sql = \"SELECT * FROM Tweetwordcount ORDER BY count DESC LIMIT 20;\"\n",
    "    \n",
    "    cur.execute(sql)\n",
    "    result = cur.fetchall()\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    # Print results\n",
    "    print 'Top-20'\n",
    "    print \"%s %8s  %s\"%('ID','Word','Count')\n",
    "    rank = 1\n",
    "    words = []\n",
    "    counts = []\n",
    "    for w,c in result:\n",
    "        print \"%2d  %8s  %d\"%(rank,w,c)\n",
    "        words.append(w)\n",
    "        counts.append(c)\n",
    "        rank += 1\n",
    "\n",
    "    bar_plot(words, counts)\n",
    "\n",
    "# ===================================================================================\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    Get top-20 words by count\n",
    "       python top20.py\n",
    "    '''    \n",
    "    top20()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
